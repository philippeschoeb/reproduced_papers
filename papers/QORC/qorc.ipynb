{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum optical reservoir computing powered by boson sampling\n",
    "\n",
    "This approach builds upon prior work by [Sakurai and al.](https://opg.optica.org/opticaq/abstract.cfm?URI=opticaq-3-3-238).\n",
    "\n",
    "This notebook demonstrates the **Quantum Optical Reservoir Computing (QORC)** experiment using the _MerLin_ quantum machine learning framework. The code replicates the performance results of quantum feature-based classification on the MNIST dataset, showcasing the proof-of-concept benefits of quantum reservoirs for machine learning tasks.\n",
    "\n",
    "QORC leverages **boson sampling** to compute highly non-linear quantum features from input data. These features, generated through photonic circuits, capture complex patterns that are difficult to extract classically. A classical linear layer is then trained on these quantum-encoded features, forming a hybrid quantum-classical classifier. This approach demonstrates how quantum reservoirs can outperform classical feature extraction by leveraging the exponential expressivity of photonic circuits, enabling enhanced classification performance on complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T20:44:33.420179Z",
     "start_time": "2025-08-29T20:44:33.409380Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main parameters\n",
    "n_photons = 3\n",
    "n_modes = 20\n",
    "seed = 42\n",
    "\n",
    "# Dataset parameters\n",
    "dataset_name = \"mnist\"\n",
    "fold_index = 0\n",
    "n_fold = 5\n",
    "\n",
    "# Training parameters\n",
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.05\n",
    "reduce_lr_patience = 10\n",
    "reduce_lr_factor = 0.5\n",
    "num_workers = 0\n",
    "pin_memory = False\n",
    "f_out_weights= \"f_weights_out.pth\"\n",
    "\n",
    "# Other parameters\n",
    "b_no_bunching = True\n",
    "b_use_tensorboard = False\n",
    "device_name = \"cpu\"\n",
    "outdir = \"outdir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Parameters explanation\n",
    "\n",
    "#### Photonic Circuit Parameters\n",
    "\n",
    "- **n_modes** : Defines the number of modes in the photonic circuit, analogous to the width of a data bus in classical computing. Photons propagate through linear optical components (e.g., phase shifters and beamsplitters).\n",
    "\n",
    "- **n_photons** :\n",
    "  Number of photons injected into the circuit. The expressiveness of the quantum layer grows exponentially with this parameter, as it directly determines the dimensionality of the output space—specifically, the number of possible Fock states (photon number distributions across modes).\n",
    "\n",
    "- **b_no_bunching** :\n",
    "  Enforces the constraint that at most one photon is detected per mode (output channel). This simplifies computation by excluding photon bunching cases. It is a valid approximation when the number of photons squared (**n_photons^2**) is significantly smaller than the number of modes (n_modes), ensuring that the probability of bunching remains negligible\n",
    "\n",
    "---\n",
    "\n",
    "#### Reproducibility\n",
    "- **seed** :\n",
    "  Controls the reproducibility of the experiment by fixing random number generators for Python, NumPy, PyTorch, and CUDA. If negative, then RNGs are not seeded.\n",
    "\n",
    "---\n",
    "\n",
    "#### Data\n",
    "- **dataset_name** :\n",
    "  Select the dataset among 'mnist' (digits), 'k-mnist' (Kuzushiji), or 'fashion-mnist' (clothing). \n",
    "\n",
    "- **fold_index** :\n",
    "  Indicates which fold of the dataset is used for validation during cross-validation.\n",
    "\n",
    "- **n_fold** :\n",
    "  Specifies the number of folds for cross-validation, determining how the dataset is split into training and validation sets.\n",
    "\n",
    "---\n",
    "\n",
    "#### Training Parameters\n",
    "- **n_epochs** :\n",
    "  Sets the number of training epochs (full passes through the dataset).\n",
    "\n",
    "- **batch_size** :\n",
    "  Number of images processed simultaneously in each stochastic gradient descent iteration.\n",
    "\n",
    "- **learning_rate** :\n",
    "  Step size for parameter updates. Higher values speed up learning but may reduce stability, while lower values ensure stability but may slow convergence.\n",
    "  \n",
    "The training parameters were selected to match those reported in the reference article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Librairies loading & logging configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T20:52:29.135845Z",
     "start_time": "2025-08-29T20:52:29.121775Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import perceval as pcvl\n",
    "import merlin as ML\n",
    "\n",
    "from lib.lib_datasets import (\n",
    "    tensor_dataset,\n",
    "    get_dataloader,\n",
    "    split_fold_numpy,\n",
    "    get_mnist_variant,\n",
    ")\n",
    "from lib.lib_learning import get_device, model_eval, model_fit\n",
    "from implementation import configure_logging\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "# Activate logging\n",
    "configure_logging(\"info\")  # basic console logging before config is resolved\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Prepare output directory with timestamped run folder\n",
    "timestamp = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "base_out = Path(outdir)\n",
    "run_dir = base_out / f\"run_{timestamp}\"\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configure logging based on resolved config and add file handler in the run directory\n",
    "configure_logging(\"info\", run_dir / \"run.log\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MNIST Dataset loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _get_mnist_variant()_ function loads one of three datasets: MNIST, Kuzushiji-MNIST (KMNIST), or Fashion-MNIST. Use the argument dataset_name with values 'mnist', 'k-mnist', or 'fashion-mnist' to specify the desired dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T20:39:44.271567Z",
     "start_time": "2025-08-29T20:36:03.254559Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 15:59:30 | INFO | __main__ | Call to qorc_encoding_and_linear_training: n_photons=3, n_modes=20, run_seed=42, fold_index=0\n",
      "2025-10-23 15:59:30 | INFO | __main__ | Loading MNIST-variant data (mnist)\n",
      "2025-10-23 15:59:31 | INFO | __main__ | Datasets sizes:\n",
      "2025-10-23 15:59:31 | INFO | __main__ | (48000,)\n",
      "2025-10-23 15:59:31 | INFO | __main__ | (48000, 784)\n",
      "2025-10-23 15:59:31 | INFO | __main__ | (12000,)\n",
      "2025-10-23 15:59:31 | INFO | __main__ | (12000, 784)\n",
      "2025-10-23 15:59:31 | INFO | __main__ | (10000,)\n",
      "2025-10-23 15:59:31 | INFO | __main__ | (10000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "storage_device = torch.device(\"cpu\")\n",
    "compute_device = get_device(device_name)\n",
    "\n",
    "run_seed = seed\n",
    "if run_seed >= 0:\n",
    "    # Seeding to control the random generators\n",
    "    random.seed(run_seed)\n",
    "    np.random.seed(run_seed)\n",
    "    torch.manual_seed(seed=run_seed)\n",
    "    torch.cuda.manual_seed_all(seed=run_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.use_deterministic_algorithms(mode=False)\n",
    "\n",
    "logger.info(\n",
    "    \"Call to qorc_encoding_and_linear_training: n_photons={}, n_modes={}, run_seed={}, fold_index={}\".format(\n",
    "        n_photons, n_modes, run_seed, fold_index\n",
    "    )\n",
    ")\n",
    "time_t1 = time.time()\n",
    "\n",
    "logger.info(\"Loading MNIST-variant data ({})\".format(dataset_name))\n",
    "val_train_data, val_train_label, test_data, test_label = get_mnist_variant(\n",
    "    dataset_name\n",
    ")\n",
    "\n",
    "val_train_data = (\n",
    "    val_train_data.reshape(val_train_data.shape[0], -1).astype(np.float32) / 255.0\n",
    ")\n",
    "\n",
    "val_label, val_data, train_label, train_data = split_fold_numpy(\n",
    "    val_train_label, val_train_data, n_fold, fold_index, split_seed=run_seed\n",
    ")\n",
    "\n",
    "test_data = test_data.reshape(test_data.shape[0], -1).astype(np.float32) / 255.0\n",
    "n_pixels = 28 * 28  # MNIST images size\n",
    "n_classes = 10  # 10 classes, one for each figure\n",
    "\n",
    "logger.info(\"Datasets sizes:\")\n",
    "logger.info(train_label.shape)  # (48000,)\n",
    "logger.info(train_data.shape)  # (48000, 784)\n",
    "logger.info(val_label.shape)  # (12000,)\n",
    "logger.info(val_data.shape)  # (12000, 784)\n",
    "logger.info(test_label.shape)  # (10000,)\n",
    "logger.info(test_data.shape)  # (10000, 784)\n",
    "\n",
    "# Display the first 5 images from the training set\n",
    "plt.figure(figsize=(12, 3))  # Width x Height in inches\n",
    "for i in range(5):\n",
    "    # Reshape flattened data (784,) into a 28x28 image\n",
    "    image = train_data[i].reshape(28, 28)\n",
    "    plt.subplot(1, 5, i + 1)  # 1 row, 5 columns, position i+1\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f\"Label: {train_label[i]}\")\n",
    "    plt.axis('off')  # Hide axes\n",
    "plt.tight_layout()  # Adjust spacing between images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Definition of the Quantum Circuit and the Quantum Layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum Circuit\n",
    "\n",
    "In the QORC experiment, the architecture comprises a pre-circuit—an M-mode random Haar-uniform interferometer with N single-photon inputs—that generates a high-dimensional photonic resource state by distributing the photons across the modes. This state is then modulated by a column of phase shifters, which encode the classical input data (e.g., PCA-transformed features) into the quantum circuit by adjusting the optical phases. The processed state is further transformed by a reservoir, implemented as a second M-mode random interferometer (often identical to the pre-circuit), which enhances the non-linearity of the quantum features. The final output, a distribution over Fock states, serves as a highly expressive feature vector for subsequent classical processing, such as linear classification. The combination of the pre-circuit, phase shifters, and reservoir enables the mapping of input data into a complex quantum feature space, where the dimensionality scales combinatorially with the number of photons and modes, offering a potential advantage over classical feature extraction methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unitary = pcvl.Matrix.random_unitary(n_modes)  # Haar-uniform unitary sampling\n",
    "precircuit = pcvl.Unitary(unitary)\n",
    "reservoir = precircuit.copy()\n",
    "\n",
    "# Input Phase Shifters\n",
    "c_var = pcvl.Circuit(n_modes)\n",
    "for i in range(n_modes):\n",
    "    px = pcvl.P(f\"px{i + 1}\")\n",
    "    port_range = i\n",
    "    c_var.add(port_range, pcvl.PS(px))\n",
    "\n",
    "qorc_circuit = precircuit // c_var // reservoir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum Layer\n",
    "The input data is encoded into the phase parameters of the intermediate phase shifters, modulating the optical modes of the photonic circuit. The quantum layer's output consists of Fock-state probabilities (ML.OutputMappingStrategy.NONE), measured via coincidence detection at the circuit's output ports. These probabilities form a high-dimensional, non-linear feature vector, which is then used as input for classical classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T21:27:43.213442Z",
     "start_time": "2025-08-29T21:21:26.007062Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 15:59:51 | INFO | __main__ | MerLin QuantumLayer creation:\n",
      "2025-10-23 15:59:51 | INFO | __main__ | QuantumLayer(custom_circuit, input_size=20, output_size=1140)\n"
     ]
    }
   ],
   "source": [
    "assert n_photons <= n_modes, (\n",
    "    \"Error with photons_input_mode: Too many photons versus modes.\"\n",
    ")\n",
    "step = (n_modes - 1) / (n_photons - 1) if n_photons > 1 else 0\n",
    "qorc_input_state = [0] * n_modes\n",
    "for k in range(n_photons):\n",
    "    index = int(round(k * step))\n",
    "    qorc_input_state[index] = 1\n",
    "\n",
    "params_prefix = [\"px\"]\n",
    "\n",
    "if b_no_bunching:\n",
    "    qorc_output_size = math.comb(n_modes, n_photons)\n",
    "else:\n",
    "    qorc_output_size = math.comb(n_photons + n_modes - 1, n_photons)\n",
    "\n",
    "logger.info(\"MerLin QuantumLayer creation:\")\n",
    "qorc_quantum_layer = ML.QuantumLayer(\n",
    "    input_size=n_modes,  # Nb input features = nb modes\n",
    "    output_size=qorc_output_size,  # Nb output classes = nb modes\n",
    "    circuit=qorc_circuit,  # QORC quantum circuit\n",
    "    trainable_parameters=[],  # Circuit is not trainable\n",
    "    input_parameters=params_prefix,  # Input encoding parameters\n",
    "    input_state=qorc_input_state,  # Initial photon state\n",
    "    output_mapping_strategy=ML.OutputMappingStrategy.NONE,  # Output: Get all Fock states probas\n",
    "    # See: https://merlinquantum.ai/user_guide/output_mappings.html\n",
    "    no_bunching=b_no_bunching,\n",
    "    device=torch.device(device_name),\n",
    ")\n",
    "logger.info(str(qorc_quantum_layer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quantum features computation\n",
    "First, **PCA (Principal Component Analysis)** is applied to reduce the dimensionality of the input data, matching it to the number of modes in the quantum circuit. The resulting PCA components are then normalized using a **global min-max scaling**, ensuring compatibility with the phase shifters in the MerLin framework, which require bounded input values (to the range [0, 1]). After encoding the normalized data with the quantm layer, the output Fock-state probabilities are further standardized using a **StandardScaler**. This final normalization step improves the convergence and performance of the subsequent **linear classification layer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 16:00:05 | INFO | __main__ | Creation of the encoder of the quantum reservoir...\n",
      "2025-10-23 16:00:06 | INFO | __main__ | Quantum features size: 1140\n",
      "2025-10-23 16:00:06 | INFO | __main__ | Encoding of the PCA comps to quantum features...\n",
      "2025-10-23 16:00:13 | INFO | __main__ | Encoding over.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Creation of the encoder of the quantum reservoir...\")\n",
    "\n",
    "# 1) PCA Components computation\n",
    "pca = PCA(n_components=n_modes)\n",
    "train_data_pca = pca.fit_transform(train_data)\n",
    "val_data_pca = pca.transform(val_data)\n",
    "test_data_pca = pca.transform(test_data)\n",
    "\n",
    "# 2) PCA comp normalization (to [0, 1] (global min/max) )\n",
    "pca_train_global_min = train_data_pca.min()\n",
    "pca_train_global_max = train_data_pca.max()\n",
    "\n",
    "def normalize_global_min_max(data, global_min, global_max):\n",
    "    epsilon = 1e-8  # Avoid zero division\n",
    "    return (data - global_min) / (global_max - global_min + epsilon)\n",
    "\n",
    "train_data_pca_norm = normalize_global_min_max(\n",
    "    train_data_pca, pca_train_global_min, pca_train_global_max\n",
    ")\n",
    "val_data_pca_norm = normalize_global_min_max(\n",
    "    val_data_pca, pca_train_global_min, pca_train_global_max\n",
    ")\n",
    "test_data_pca_norm = normalize_global_min_max(\n",
    "    test_data_pca, pca_train_global_min, pca_train_global_max\n",
    ")\n",
    "\n",
    "logger.info(\"Quantum features size: {}\".format(qorc_output_size))\n",
    "logger.info(\"Encoding of the PCA comps to quantum features...\")\n",
    "time_t2 = time.time()\n",
    "train_data_qorc = qorc_quantum_layer(\n",
    "    torch.tensor(train_data_pca_norm, dtype=torch.float32, device=compute_device)\n",
    ")\n",
    "val_data_qorc = qorc_quantum_layer(\n",
    "    torch.tensor(val_data_pca_norm, dtype=torch.float32, device=compute_device)\n",
    ")\n",
    "test_data_qorc = qorc_quantum_layer(\n",
    "    torch.tensor(test_data_pca_norm, dtype=torch.float32, device=compute_device)\n",
    ")\n",
    "logger.info(\"Encoding over.\")\n",
    "time_t3 = time.time()\n",
    "\n",
    "# 4) Quantum features normalization (standard_scaler)\n",
    "qorc_train_mean = train_data_qorc.detach().mean(dim=0)\n",
    "qorc_train_std = train_data_qorc.detach().std(dim=0)\n",
    "\n",
    "def normalize_standard_scaler(data, mean, std):\n",
    "    epsilon = 1e-8  # Avoid zero division\n",
    "    return (data - mean) / (std + epsilon)\n",
    "\n",
    "train_data_qorc_norm = normalize_standard_scaler(\n",
    "    train_data_qorc, qorc_train_mean, qorc_train_std\n",
    ")\n",
    "val_data_qorc_norm = normalize_standard_scaler(\n",
    "    val_data_qorc, qorc_train_mean, qorc_train_std\n",
    ")\n",
    "test_data_qorc_norm = normalize_standard_scaler(\n",
    "    test_data_qorc, qorc_train_mean, qorc_train_std\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Datasets and dataloader preparation for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of the linear layer is the concatenation of the MNIST images and the quantum features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T21:00:56.614353Z",
     "start_time": "2025-08-29T20:57:24.903483Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 16:00:28 | INFO | __main__ | train dataset len: 48000\n",
      "2025-10-23 16:00:28 | INFO | __main__ | val dataset len  : 12000\n",
      "2025-10-23 16:00:28 | INFO | __main__ | test dataset len : 10000\n",
      "2025-10-23 16:00:28 | INFO | __main__ | train loader len: 480\n",
      "2025-10-23 16:00:28 | INFO | __main__ | val loader len  : 120\n",
      "2025-10-23 16:00:28 | INFO | __main__ | test loader len : 100\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "all_train_data = torch.cat(\n",
    "    (\n",
    "        torch.tensor(train_data, dtype=dtype, device=compute_device),\n",
    "        train_data_qorc_norm,\n",
    "    ),\n",
    "    dim=1,\n",
    ")\n",
    "all_val_data = torch.cat(\n",
    "    (\n",
    "        torch.tensor(val_data, dtype=dtype, device=compute_device),\n",
    "        val_data_qorc_norm,\n",
    "    ),\n",
    "    dim=1,\n",
    ")\n",
    "all_test_data = torch.cat(\n",
    "    (\n",
    "        torch.tensor(test_data, dtype=dtype, device=compute_device),\n",
    "        test_data_qorc_norm,\n",
    "    ),\n",
    "    dim=1,\n",
    ")\n",
    "\n",
    "# Datasets\n",
    "ds_train = tensor_dataset(\n",
    "    all_train_data,\n",
    "    train_label,\n",
    "    storage_device,\n",
    "    dtype=torch.float32,\n",
    "    transform=None,\n",
    "    n_side_pixels=28,\n",
    ")\n",
    "ds_val = tensor_dataset(\n",
    "    all_val_data, val_label, storage_device, dtype=torch.float32\n",
    ")\n",
    "ds_test = tensor_dataset(\n",
    "    all_test_data, test_label, storage_device, dtype=torch.float32\n",
    ")\n",
    "\n",
    "logger.info(\"train dataset len: {}\".format(len(ds_train)))\n",
    "logger.info(\"val dataset len  : {}\".format(len(ds_val)))\n",
    "logger.info(\"test dataset len : {}\".format(len(ds_test)))\n",
    "\n",
    "# Dataloaders\n",
    "shuffle_train = True\n",
    "shuffle_test = False\n",
    "train_loader = get_dataloader(\n",
    "    ds_train, batch_size, shuffle_train, num_workers, pin_memory, run_seed\n",
    ")\n",
    "val_loader = get_dataloader(\n",
    "    ds_val, batch_size, shuffle_test, num_workers, pin_memory, run_seed\n",
    ")\n",
    "test_loader = get_dataloader(\n",
    "    ds_test, batch_size, shuffle_test, num_workers, pin_memory, run_seed\n",
    ")\n",
    "\n",
    "logger.info(\"train loader len: {}\".format(len(train_loader)))\n",
    "logger.info(\"val loader len  : {}\".format(len(val_loader)))\n",
    "logger.info(\"test loader len : {}\".format(len(test_loader)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Definition: Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model consists of a standard PyTorch linear layer (nn.Linear), used to evaluate the quality of the quantum features by measuring the test accuracy. This simple classifier demonstrates how effectively the quantum reservoir transforms the input data into discriminative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T21:09:06.454017Z",
     "start_time": "2025-08-29T21:02:52.172014Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 16:00:42 | INFO | __main__ | Prepare the linear classifier\n",
      "2025-10-23 16:00:42 | INFO | __main__ | n_model_input_features: 1924\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Prepare the linear classifier\")\n",
    "\n",
    "n_model_input_features = n_pixels + qorc_output_size\n",
    "logger.info(\"n_model_input_features: {}\".format(n_model_input_features))\n",
    "linear = nn.Linear(\n",
    "    n_model_input_features, n_classes, bias=True, device=compute_device\n",
    ")\n",
    "\n",
    "nn.init.xavier_uniform_(linear.weight)  # Xavier uniforme init (Glorot)\n",
    "nn.init.zeros_(linear.bias)\n",
    "model = linear\n",
    "model.to(compute_device)\n",
    "model.train()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Launching the Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the reference article, the AdaGrad optimizer is used for training. The validation set is exclusively used for model selection based on the lowest validation loss. Consequently, no early stopping strategy is applied in this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T21:12:24.847196Z",
     "start_time": "2025-08-29T21:12:24.836279Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 16:01:12 | INFO | __main__ | Evaluation before training (on test set)\n",
      "2025-10-23 16:01:13 | INFO | __main__ | 100/100 - 0s - loss: 2.9305 - accuracy: 0.0624\n",
      "2025-10-23 16:01:13 | INFO | __main__ | Beginning of training\n",
      "2025-10-23 16:01:16 | INFO | __main__ | Call model_fit(), with 19250 parameters to train.\n",
      "2025-10-23 16:01:16 | INFO | __main__ | tag heure: 2025-10-23_16:01:16 (1761228076.38814)\n",
      "2025-10-23 16:01:20 | INFO | __main__ | Epoch 1/100\n",
      "2025-10-23 16:01:20 | INFO | __main__ | 480/480 - 4s - loss: 0.3377 - accuracy: 0.9246 - val_loss: 0.1640 - val_accuracy: 0.9535\n",
      "2025-10-23 16:01:20 | INFO | __main__ | Epoch 00001: val_loss improved from inf to 0.16398, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:01:20 | INFO | __main__ | tag heure: 2025-10-23_16:01:20 (1761228080.6782465)\n",
      "2025-10-23 16:01:24 | INFO | __main__ | Epoch 2/100\n",
      "2025-10-23 16:01:24 | INFO | __main__ | 480/480 - 3s - loss: 0.1378 - accuracy: 0.9586 - val_loss: 0.1380 - val_accuracy: 0.9563\n",
      "2025-10-23 16:01:24 | INFO | __main__ | Epoch 00002: val_loss improved from 0.16398 to 0.13800, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:01:24 | INFO | __main__ | tag heure: 2025-10-23_16:01:24 (1761228084.6824129)\n",
      "2025-10-23 16:01:28 | INFO | __main__ | Epoch 3/100\n",
      "2025-10-23 16:01:28 | INFO | __main__ | 480/480 - 4s - loss: 0.1121 - accuracy: 0.9660 - val_loss: 0.1258 - val_accuracy: 0.9622\n",
      "2025-10-23 16:01:28 | INFO | __main__ | Epoch 00003: val_loss improved from 0.13800 to 0.12576, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:01:28 | INFO | __main__ | tag heure: 2025-10-23_16:01:28 (1761228088.807986)\n",
      "2025-10-23 16:01:33 | INFO | __main__ | Epoch 4/100\n",
      "2025-10-23 16:01:33 | INFO | __main__ | 480/480 - 4s - loss: 0.1003 - accuracy: 0.9697 - val_loss: 0.1147 - val_accuracy: 0.9652\n",
      "2025-10-23 16:01:33 | INFO | __main__ | Epoch 00004: val_loss improved from 0.12576 to 0.11472, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:01:33 | INFO | __main__ | tag heure: 2025-10-23_16:01:33 (1761228093.0183465)\n",
      "2025-10-23 16:01:37 | INFO | __main__ | Epoch 5/100\n",
      "2025-10-23 16:01:37 | INFO | __main__ | 480/480 - 4s - loss: 0.0915 - accuracy: 0.9719 - val_loss: 0.1126 - val_accuracy: 0.9655\n",
      "2025-10-23 16:01:37 | INFO | __main__ | Epoch 00005: val_loss improved from 0.11472 to 0.11256, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:01:37 | INFO | __main__ | tag heure: 2025-10-23_16:01:37 (1761228097.1037965)\n",
      "2025-10-23 16:01:40 | INFO | __main__ | Epoch 6/100\n",
      "2025-10-23 16:01:40 | INFO | __main__ | 480/480 - 3s - loss: 0.0860 - accuracy: 0.9737 - val_loss: 0.1123 - val_accuracy: 0.9653\n",
      "2025-10-23 16:01:41 | INFO | __main__ | Epoch 00006: val_loss improved from 0.11256 to 0.11234, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:01:41 | INFO | __main__ | tag heure: 2025-10-23_16:01:41 (1761228101.011497)\n",
      "2025-10-23 16:01:44 | INFO | __main__ | Epoch 7/100\n",
      "2025-10-23 16:01:44 | INFO | __main__ | 480/480 - 3s - loss: 0.0808 - accuracy: 0.9749 - val_loss: 0.1060 - val_accuracy: 0.9672\n",
      "2025-10-23 16:01:44 | INFO | __main__ | Epoch 00007: val_loss improved from 0.11234 to 0.10597, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:01:44 | INFO | __main__ | tag heure: 2025-10-23_16:01:44 (1761228104.8316512)\n",
      "2025-10-23 16:01:49 | INFO | __main__ | Epoch 8/100\n",
      "2025-10-23 16:01:49 | INFO | __main__ | 480/480 - 4s - loss: 0.0771 - accuracy: 0.9766 - val_loss: 0.1075 - val_accuracy: 0.9671\n",
      "2025-10-23 16:01:49 | INFO | __main__ | Epoch 00008: val_loss did not improve from 0.10597\n",
      "2025-10-23 16:01:49 | INFO | __main__ | tag heure: 2025-10-23_16:01:49 (1761228109.3779733)\n",
      "2025-10-23 16:01:53 | INFO | __main__ | Epoch 9/100\n",
      "2025-10-23 16:01:53 | INFO | __main__ | 480/480 - 4s - loss: 0.0738 - accuracy: 0.9771 - val_loss: 0.1087 - val_accuracy: 0.9655\n",
      "2025-10-23 16:01:53 | INFO | __main__ | Epoch 00009: val_loss did not improve from 0.10597\n",
      "2025-10-23 16:01:53 | INFO | __main__ | tag heure: 2025-10-23_16:01:53 (1761228113.628095)\n",
      "2025-10-23 16:01:58 | INFO | __main__ | Epoch 10/100\n",
      "2025-10-23 16:01:58 | INFO | __main__ | 480/480 - 4s - loss: 0.0718 - accuracy: 0.9778 - val_loss: 0.1021 - val_accuracy: 0.9692\n",
      "2025-10-23 16:01:58 | INFO | __main__ | Epoch 00010: val_loss improved from 0.10597 to 0.10206, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:01:58 | INFO | __main__ | tag heure: 2025-10-23_16:01:58 (1761228118.0379899)\n",
      "2025-10-23 16:02:01 | INFO | __main__ | Epoch 11/100\n",
      "2025-10-23 16:02:01 | INFO | __main__ | 480/480 - 3s - loss: 0.0688 - accuracy: 0.9786 - val_loss: 0.1016 - val_accuracy: 0.9691\n",
      "2025-10-23 16:02:01 | INFO | __main__ | Epoch 00011: val_loss improved from 0.10206 to 0.10165, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:02:01 | INFO | __main__ | tag heure: 2025-10-23_16:02:01 (1761228121.8091924)\n",
      "2025-10-23 16:02:05 | INFO | __main__ | Epoch 12/100\n",
      "2025-10-23 16:02:05 | INFO | __main__ | 480/480 - 3s - loss: 0.0674 - accuracy: 0.9795 - val_loss: 0.1018 - val_accuracy: 0.9680\n",
      "2025-10-23 16:02:05 | INFO | __main__ | Epoch 00012: val_loss did not improve from 0.10165\n",
      "2025-10-23 16:02:05 | INFO | __main__ | tag heure: 2025-10-23_16:02:05 (1761228125.222088)\n",
      "2025-10-23 16:02:09 | INFO | __main__ | Epoch 13/100\n",
      "2025-10-23 16:02:09 | INFO | __main__ | 480/480 - 3s - loss: 0.0657 - accuracy: 0.9803 - val_loss: 0.0996 - val_accuracy: 0.9679\n",
      "2025-10-23 16:02:09 | INFO | __main__ | Epoch 00013: val_loss improved from 0.10165 to 0.09961, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:02:09 | INFO | __main__ | tag heure: 2025-10-23_16:02:09 (1761228129.023073)\n",
      "2025-10-23 16:02:13 | INFO | __main__ | Epoch 14/100\n",
      "2025-10-23 16:02:13 | INFO | __main__ | 480/480 - 4s - loss: 0.0637 - accuracy: 0.9805 - val_loss: 0.1000 - val_accuracy: 0.9687\n",
      "2025-10-23 16:02:13 | INFO | __main__ | Epoch 00014: val_loss did not improve from 0.09961\n",
      "2025-10-23 16:02:13 | INFO | __main__ | tag heure: 2025-10-23_16:02:13 (1761228133.297999)\n",
      "2025-10-23 16:02:17 | INFO | __main__ | Epoch 15/100\n",
      "2025-10-23 16:02:17 | INFO | __main__ | 480/480 - 3s - loss: 0.0618 - accuracy: 0.9815 - val_loss: 0.0996 - val_accuracy: 0.9690\n",
      "2025-10-23 16:02:17 | INFO | __main__ | Epoch 00015: val_loss improved from 0.09961 to 0.09961, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:02:17 | INFO | __main__ | tag heure: 2025-10-23_16:02:17 (1761228137.2680328)\n",
      "2025-10-23 16:02:20 | INFO | __main__ | Epoch 16/100\n",
      "2025-10-23 16:02:20 | INFO | __main__ | 480/480 - 3s - loss: 0.0610 - accuracy: 0.9817 - val_loss: 0.0993 - val_accuracy: 0.9693\n",
      "2025-10-23 16:02:20 | INFO | __main__ | Epoch 00016: val_loss improved from 0.09961 to 0.09932, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:02:20 | INFO | __main__ | tag heure: 2025-10-23_16:02:20 (1761228140.8869076)\n",
      "2025-10-23 16:02:24 | INFO | __main__ | Epoch 17/100\n",
      "2025-10-23 16:02:24 | INFO | __main__ | 480/480 - 3s - loss: 0.0594 - accuracy: 0.9823 - val_loss: 0.0993 - val_accuracy: 0.9690\n",
      "2025-10-23 16:02:24 | INFO | __main__ | Epoch 00017: val_loss did not improve from 0.09932\n",
      "2025-10-23 16:02:24 | INFO | __main__ | tag heure: 2025-10-23_16:02:24 (1761228144.3542054)\n",
      "2025-10-23 16:02:28 | INFO | __main__ | Epoch 18/100\n",
      "2025-10-23 16:02:28 | INFO | __main__ | 480/480 - 4s - loss: 0.0583 - accuracy: 0.9828 - val_loss: 0.0985 - val_accuracy: 0.9696\n",
      "2025-10-23 16:02:28 | INFO | __main__ | Epoch 00018: val_loss improved from 0.09932 to 0.09850, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:02:28 | INFO | __main__ | tag heure: 2025-10-23_16:02:28 (1761228148.4702272)\n",
      "2025-10-23 16:02:32 | INFO | __main__ | Epoch 19/100\n",
      "2025-10-23 16:02:32 | INFO | __main__ | 480/480 - 4s - loss: 0.0574 - accuracy: 0.9826 - val_loss: 0.0990 - val_accuracy: 0.9690\n",
      "2025-10-23 16:02:32 | INFO | __main__ | Epoch 00019: val_loss did not improve from 0.09850\n",
      "2025-10-23 16:02:32 | INFO | __main__ | tag heure: 2025-10-23_16:02:32 (1761228152.5381951)\n",
      "2025-10-23 16:02:36 | INFO | __main__ | Epoch 20/100\n",
      "2025-10-23 16:02:36 | INFO | __main__ | 480/480 - 3s - loss: 0.0560 - accuracy: 0.9832 - val_loss: 0.0961 - val_accuracy: 0.9698\n",
      "2025-10-23 16:02:36 | INFO | __main__ | Epoch 00020: val_loss improved from 0.09850 to 0.09607, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:02:36 | INFO | __main__ | tag heure: 2025-10-23_16:02:36 (1761228156.4581606)\n",
      "2025-10-23 16:02:40 | INFO | __main__ | Epoch 21/100\n",
      "2025-10-23 16:02:40 | INFO | __main__ | 480/480 - 3s - loss: 0.0555 - accuracy: 0.9838 - val_loss: 0.0982 - val_accuracy: 0.9687\n",
      "2025-10-23 16:02:40 | INFO | __main__ | Epoch 00021: val_loss did not improve from 0.09607\n",
      "2025-10-23 16:02:40 | INFO | __main__ | tag heure: 2025-10-23_16:02:40 (1761228160.1030445)\n",
      "2025-10-23 16:02:44 | INFO | __main__ | Epoch 22/100\n",
      "2025-10-23 16:02:44 | INFO | __main__ | 480/480 - 4s - loss: 0.0542 - accuracy: 0.9837 - val_loss: 0.1005 - val_accuracy: 0.9685\n",
      "2025-10-23 16:02:44 | INFO | __main__ | Epoch 00022: val_loss did not improve from 0.09607\n",
      "2025-10-23 16:02:44 | INFO | __main__ | tag heure: 2025-10-23_16:02:44 (1761228164.343733)\n",
      "2025-10-23 16:02:48 | INFO | __main__ | Epoch 23/100\n",
      "2025-10-23 16:02:48 | INFO | __main__ | 480/480 - 3s - loss: 0.0534 - accuracy: 0.9846 - val_loss: 0.0999 - val_accuracy: 0.9689\n",
      "2025-10-23 16:02:48 | INFO | __main__ | Epoch 00023: val_loss did not improve from 0.09607\n",
      "2025-10-23 16:02:48 | INFO | __main__ | tag heure: 2025-10-23_16:02:48 (1761228168.1881843)\n",
      "2025-10-23 16:02:52 | INFO | __main__ | Epoch 24/100\n",
      "2025-10-23 16:02:52 | INFO | __main__ | 480/480 - 4s - loss: 0.0526 - accuracy: 0.9845 - val_loss: 0.0965 - val_accuracy: 0.9703\n",
      "2025-10-23 16:02:52 | INFO | __main__ | Epoch 00024: val_loss did not improve from 0.09607\n",
      "2025-10-23 16:02:52 | INFO | __main__ | tag heure: 2025-10-23_16:02:52 (1761228172.3479764)\n",
      "2025-10-23 16:02:56 | INFO | __main__ | Epoch 25/100\n",
      "2025-10-23 16:02:56 | INFO | __main__ | 480/480 - 4s - loss: 0.0518 - accuracy: 0.9848 - val_loss: 0.0975 - val_accuracy: 0.9698\n",
      "2025-10-23 16:02:56 | INFO | __main__ | Epoch 00025: val_loss did not improve from 0.09607\n",
      "2025-10-23 16:02:56 | INFO | __main__ | tag heure: 2025-10-23_16:02:56 (1761228176.4610379)\n",
      "2025-10-23 16:03:00 | INFO | __main__ | Epoch 26/100\n",
      "2025-10-23 16:03:00 | INFO | __main__ | 480/480 - 4s - loss: 0.0513 - accuracy: 0.9853 - val_loss: 0.0979 - val_accuracy: 0.9697\n",
      "2025-10-23 16:03:00 | INFO | __main__ | Epoch 00026: val_loss did not improve from 0.09607\n",
      "2025-10-23 16:03:00 | INFO | __main__ | tag heure: 2025-10-23_16:03:00 (1761228180.9380379)\n",
      "2025-10-23 16:03:05 | INFO | __main__ | Epoch 27/100\n",
      "2025-10-23 16:03:05 | INFO | __main__ | 480/480 - 4s - loss: 0.0506 - accuracy: 0.9852 - val_loss: 0.0965 - val_accuracy: 0.9703\n",
      "2025-10-23 16:03:05 | INFO | __main__ | Epoch 00027: val_loss did not improve from 0.09607\n",
      "2025-10-23 16:03:05 | INFO | __main__ | tag heure: 2025-10-23_16:03:05 (1761228185.0097473)\n",
      "2025-10-23 16:03:09 | INFO | __main__ | Epoch 28/100\n",
      "2025-10-23 16:03:09 | INFO | __main__ | 480/480 - 4s - loss: 0.0498 - accuracy: 0.9850 - val_loss: 0.0960 - val_accuracy: 0.9702\n",
      "2025-10-23 16:03:09 | INFO | __main__ | Epoch 00028: val_loss improved from 0.09607 to 0.09600, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:03:09 | INFO | __main__ | tag heure: 2025-10-23_16:03:09 (1761228189.0982313)\n",
      "2025-10-23 16:03:12 | INFO | __main__ | Epoch 29/100\n",
      "2025-10-23 16:03:12 | INFO | __main__ | 480/480 - 3s - loss: 0.0491 - accuracy: 0.9857 - val_loss: 0.0961 - val_accuracy: 0.9700\n",
      "2025-10-23 16:03:12 | INFO | __main__ | Epoch 00029: val_loss did not improve from 0.09600\n",
      "2025-10-23 16:03:12 | INFO | __main__ | tag heure: 2025-10-23_16:03:12 (1761228192.340759)\n",
      "2025-10-23 16:03:16 | INFO | __main__ | Epoch 30/100\n",
      "2025-10-23 16:03:16 | INFO | __main__ | 480/480 - 3s - loss: 0.0484 - accuracy: 0.9859 - val_loss: 0.0995 - val_accuracy: 0.9691\n",
      "2025-10-23 16:03:16 | INFO | __main__ | Epoch 00030: val_loss did not improve from 0.09600\n",
      "2025-10-23 16:03:16 | INFO | __main__ | tag heure: 2025-10-23_16:03:16 (1761228196.2135105)\n",
      "2025-10-23 16:03:20 | INFO | __main__ | Epoch 31/100\n",
      "2025-10-23 16:03:20 | INFO | __main__ | 480/480 - 4s - loss: 0.0478 - accuracy: 0.9862 - val_loss: 0.0959 - val_accuracy: 0.9702\n",
      "2025-10-23 16:03:20 | INFO | __main__ | Epoch 00031: val_loss improved from 0.09600 to 0.09589, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:03:20 | INFO | __main__ | tag heure: 2025-10-23_16:03:20 (1761228200.4578593)\n",
      "2025-10-23 16:03:24 | INFO | __main__ | Epoch 32/100\n",
      "2025-10-23 16:03:24 | INFO | __main__ | 480/480 - 4s - loss: 0.0476 - accuracy: 0.9865 - val_loss: 0.0943 - val_accuracy: 0.9701\n",
      "2025-10-23 16:03:24 | INFO | __main__ | Epoch 00032: val_loss improved from 0.09589 to 0.09428, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:03:24 | INFO | __main__ | tag heure: 2025-10-23_16:03:24 (1761228204.8259227)\n",
      "2025-10-23 16:03:29 | INFO | __main__ | Epoch 33/100\n",
      "2025-10-23 16:03:29 | INFO | __main__ | 480/480 - 4s - loss: 0.0468 - accuracy: 0.9861 - val_loss: 0.0947 - val_accuracy: 0.9706\n",
      "2025-10-23 16:03:29 | INFO | __main__ | Epoch 00033: val_loss did not improve from 0.09428\n",
      "2025-10-23 16:03:29 | INFO | __main__ | tag heure: 2025-10-23_16:03:29 (1761228209.2291245)\n",
      "2025-10-23 16:03:33 | INFO | __main__ | Epoch 34/100\n",
      "2025-10-23 16:03:33 | INFO | __main__ | 480/480 - 4s - loss: 0.0462 - accuracy: 0.9866 - val_loss: 0.0959 - val_accuracy: 0.9710\n",
      "2025-10-23 16:03:33 | INFO | __main__ | Epoch 00034: val_loss did not improve from 0.09428\n",
      "2025-10-23 16:03:33 | INFO | __main__ | tag heure: 2025-10-23_16:03:33 (1761228213.445323)\n",
      "2025-10-23 16:03:37 | INFO | __main__ | Epoch 35/100\n",
      "2025-10-23 16:03:37 | INFO | __main__ | 480/480 - 4s - loss: 0.0456 - accuracy: 0.9869 - val_loss: 0.0995 - val_accuracy: 0.9680\n",
      "2025-10-23 16:03:37 | INFO | __main__ | Epoch 00035: val_loss did not improve from 0.09428\n",
      "2025-10-23 16:03:37 | INFO | __main__ | tag heure: 2025-10-23_16:03:37 (1761228217.7833724)\n",
      "2025-10-23 16:03:41 | INFO | __main__ | Epoch 36/100\n",
      "2025-10-23 16:03:41 | INFO | __main__ | 480/480 - 4s - loss: 0.0456 - accuracy: 0.9870 - val_loss: 0.0962 - val_accuracy: 0.9702\n",
      "2025-10-23 16:03:41 | INFO | __main__ | Epoch 00036: val_loss did not improve from 0.09428\n",
      "2025-10-23 16:03:41 | INFO | __main__ | tag heure: 2025-10-23_16:03:41 (1761228221.8588307)\n",
      "2025-10-23 16:03:46 | INFO | __main__ | Epoch 37/100\n",
      "2025-10-23 16:03:46 | INFO | __main__ | 480/480 - 4s - loss: 0.0450 - accuracy: 0.9873 - val_loss: 0.0973 - val_accuracy: 0.9693\n",
      "2025-10-23 16:03:46 | INFO | __main__ | Epoch 00037: val_loss did not improve from 0.09428\n",
      "2025-10-23 16:03:46 | INFO | __main__ | tag heure: 2025-10-23_16:03:46 (1761228226.221036)\n",
      "2025-10-23 16:03:49 | INFO | __main__ | Epoch 38/100\n",
      "2025-10-23 16:03:49 | INFO | __main__ | 480/480 - 3s - loss: 0.0445 - accuracy: 0.9874 - val_loss: 0.0946 - val_accuracy: 0.9700\n",
      "2025-10-23 16:03:49 | INFO | __main__ | Epoch 00038: val_loss did not improve from 0.09428\n",
      "2025-10-23 16:03:49 | INFO | __main__ | tag heure: 2025-10-23_16:03:49 (1761228229.8237038)\n",
      "2025-10-23 16:03:53 | INFO | __main__ | Epoch 39/100\n",
      "2025-10-23 16:03:53 | INFO | __main__ | 480/480 - 3s - loss: 0.0440 - accuracy: 0.9870 - val_loss: 0.0959 - val_accuracy: 0.9702\n",
      "2025-10-23 16:03:53 | INFO | __main__ | Epoch 00039: val_loss did not improve from 0.09428\n",
      "2025-10-23 16:03:53 | INFO | __main__ | tag heure: 2025-10-23_16:03:53 (1761228233.743978)\n",
      "2025-10-23 16:03:57 | INFO | __main__ | Epoch 40/100\n",
      "2025-10-23 16:03:57 | INFO | __main__ | 480/480 - 3s - loss: 0.0435 - accuracy: 0.9880 - val_loss: 0.0956 - val_accuracy: 0.9707\n",
      "2025-10-23 16:03:57 | INFO | __main__ | Epoch 00040: val_loss did not improve from 0.09428\n",
      "2025-10-23 16:03:57 | INFO | __main__ | tag heure: 2025-10-23_16:03:57 (1761228237.6081834)\n",
      "2025-10-23 16:04:01 | INFO | __main__ | Epoch 41/100\n",
      "2025-10-23 16:04:01 | INFO | __main__ | 480/480 - 4s - loss: 0.0433 - accuracy: 0.9875 - val_loss: 0.0966 - val_accuracy: 0.9698\n",
      "2025-10-23 16:04:01 | INFO | __main__ | Epoch 00041: val_loss did not improve from 0.09428\n",
      "2025-10-23 16:04:01 | INFO | __main__ | tag heure: 2025-10-23_16:04:01 (1761228241.6921344)\n",
      "2025-10-23 16:04:05 | INFO | __main__ | Epoch 42/100\n",
      "2025-10-23 16:04:05 | INFO | __main__ | 480/480 - 3s - loss: 0.0429 - accuracy: 0.9878 - val_loss: 0.0944 - val_accuracy: 0.9708\n",
      "2025-10-23 16:04:05 | INFO | __main__ | Epoch 00042: val_loss did not improve from 0.09428\n",
      "2025-10-23 16:04:05 | INFO | __main__ | Epoch 00042: ReduceLROnPlateau reducing learning rate to 2.500000e-02\n",
      "2025-10-23 16:04:05 | INFO | __main__ | tag heure: 2025-10-23_16:04:05 (1761228245.2699332)\n",
      "2025-10-23 16:04:08 | INFO | __main__ | Epoch 43/100\n",
      "2025-10-23 16:04:08 | INFO | __main__ | 480/480 - 3s - loss: 0.0405 - accuracy: 0.9891 - val_loss: 0.0937 - val_accuracy: 0.9707\n",
      "2025-10-23 16:04:08 | INFO | __main__ | Epoch 00043: val_loss improved from 0.09428 to 0.09367, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:04:08 | INFO | __main__ | tag heure: 2025-10-23_16:04:08 (1761228248.985286)\n",
      "2025-10-23 16:04:13 | INFO | __main__ | Epoch 44/100\n",
      "2025-10-23 16:04:13 | INFO | __main__ | 480/480 - 4s - loss: 0.0403 - accuracy: 0.9892 - val_loss: 0.0936 - val_accuracy: 0.9704\n",
      "2025-10-23 16:04:13 | INFO | __main__ | Epoch 00044: val_loss improved from 0.09367 to 0.09356, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:04:13 | INFO | __main__ | tag heure: 2025-10-23_16:04:13 (1761228253.2884042)\n",
      "2025-10-23 16:04:17 | INFO | __main__ | Epoch 45/100\n",
      "2025-10-23 16:04:17 | INFO | __main__ | 480/480 - 4s - loss: 0.0399 - accuracy: 0.9894 - val_loss: 0.0930 - val_accuracy: 0.9715\n",
      "2025-10-23 16:04:17 | INFO | __main__ | Epoch 00045: val_loss improved from 0.09356 to 0.09304, saving model to outdir\\run_20251023-155601\\f_weights_out.pth\n",
      "2025-10-23 16:04:17 | INFO | __main__ | tag heure: 2025-10-23_16:04:17 (1761228257.6078002)\n",
      "2025-10-23 16:04:20 | INFO | __main__ | Epoch 46/100\n",
      "2025-10-23 16:04:20 | INFO | __main__ | 480/480 - 3s - loss: 0.0399 - accuracy: 0.9894 - val_loss: 0.0940 - val_accuracy: 0.9698\n",
      "2025-10-23 16:04:20 | INFO | __main__ | Epoch 00046: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:04:20 | INFO | __main__ | tag heure: 2025-10-23_16:04:20 (1761228260.8702261)\n",
      "2025-10-23 16:04:24 | INFO | __main__ | Epoch 47/100\n",
      "2025-10-23 16:04:24 | INFO | __main__ | 480/480 - 3s - loss: 0.0395 - accuracy: 0.9896 - val_loss: 0.0932 - val_accuracy: 0.9708\n",
      "2025-10-23 16:04:24 | INFO | __main__ | Epoch 00047: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:04:24 | INFO | __main__ | tag heure: 2025-10-23_16:04:24 (1761228264.2301214)\n",
      "2025-10-23 16:04:27 | INFO | __main__ | Epoch 48/100\n",
      "2025-10-23 16:04:27 | INFO | __main__ | 480/480 - 3s - loss: 0.0395 - accuracy: 0.9899 - val_loss: 0.0947 - val_accuracy: 0.9707\n",
      "2025-10-23 16:04:27 | INFO | __main__ | Epoch 00048: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:04:27 | INFO | __main__ | tag heure: 2025-10-23_16:04:27 (1761228267.7783022)\n",
      "2025-10-23 16:04:31 | INFO | __main__ | Epoch 49/100\n",
      "2025-10-23 16:04:31 | INFO | __main__ | 480/480 - 4s - loss: 0.0394 - accuracy: 0.9900 - val_loss: 0.0937 - val_accuracy: 0.9702\n",
      "2025-10-23 16:04:31 | INFO | __main__ | Epoch 00049: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:04:31 | INFO | __main__ | tag heure: 2025-10-23_16:04:31 (1761228271.8980727)\n",
      "2025-10-23 16:04:35 | INFO | __main__ | Epoch 50/100\n",
      "2025-10-23 16:04:35 | INFO | __main__ | 480/480 - 4s - loss: 0.0390 - accuracy: 0.9900 - val_loss: 0.0934 - val_accuracy: 0.9712\n",
      "2025-10-23 16:04:35 | INFO | __main__ | Epoch 00050: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:04:36 | INFO | __main__ | tag heure: 2025-10-23_16:04:35 (1761228275.9981785)\n",
      "2025-10-23 16:04:40 | INFO | __main__ | Epoch 51/100\n",
      "2025-10-23 16:04:40 | INFO | __main__ | 480/480 - 4s - loss: 0.0389 - accuracy: 0.9900 - val_loss: 0.0935 - val_accuracy: 0.9712\n",
      "2025-10-23 16:04:40 | INFO | __main__ | Epoch 00051: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:04:40 | INFO | __main__ | tag heure: 2025-10-23_16:04:40 (1761228280.292695)\n",
      "2025-10-23 16:04:44 | INFO | __main__ | Epoch 52/100\n",
      "2025-10-23 16:04:44 | INFO | __main__ | 480/480 - 4s - loss: 0.0389 - accuracy: 0.9900 - val_loss: 0.0933 - val_accuracy: 0.9712\n",
      "2025-10-23 16:04:44 | INFO | __main__ | Epoch 00052: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:04:44 | INFO | __main__ | tag heure: 2025-10-23_16:04:44 (1761228284.6965463)\n",
      "2025-10-23 16:04:48 | INFO | __main__ | Epoch 53/100\n",
      "2025-10-23 16:04:48 | INFO | __main__ | 480/480 - 3s - loss: 0.0386 - accuracy: 0.9901 - val_loss: 0.0939 - val_accuracy: 0.9706\n",
      "2025-10-23 16:04:48 | INFO | __main__ | Epoch 00053: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:04:48 | INFO | __main__ | tag heure: 2025-10-23_16:04:48 (1761228288.5236704)\n",
      "2025-10-23 16:04:52 | INFO | __main__ | Epoch 54/100\n",
      "2025-10-23 16:04:52 | INFO | __main__ | 480/480 - 3s - loss: 0.0386 - accuracy: 0.9899 - val_loss: 0.0943 - val_accuracy: 0.9707\n",
      "2025-10-23 16:04:52 | INFO | __main__ | Epoch 00054: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:04:52 | INFO | __main__ | tag heure: 2025-10-23_16:04:52 (1761228292.2219746)\n",
      "2025-10-23 16:04:56 | INFO | __main__ | Epoch 55/100\n",
      "2025-10-23 16:04:56 | INFO | __main__ | 480/480 - 3s - loss: 0.0384 - accuracy: 0.9903 - val_loss: 0.0938 - val_accuracy: 0.9704\n",
      "2025-10-23 16:04:56 | INFO | __main__ | Epoch 00055: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:04:56 | INFO | __main__ | Epoch 00055: ReduceLROnPlateau reducing learning rate to 1.250000e-02\n",
      "2025-10-23 16:04:56 | INFO | __main__ | tag heure: 2025-10-23_16:04:56 (1761228296.1605022)\n",
      "2025-10-23 16:05:00 | INFO | __main__ | Epoch 56/100\n",
      "2025-10-23 16:05:00 | INFO | __main__ | 480/480 - 4s - loss: 0.0375 - accuracy: 0.9905 - val_loss: 0.0933 - val_accuracy: 0.9709\n",
      "2025-10-23 16:05:00 | INFO | __main__ | Epoch 00056: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:05:00 | INFO | __main__ | tag heure: 2025-10-23_16:05:00 (1761228300.32234)\n",
      "2025-10-23 16:05:04 | INFO | __main__ | Epoch 57/100\n",
      "2025-10-23 16:05:04 | INFO | __main__ | 480/480 - 4s - loss: 0.0373 - accuracy: 0.9907 - val_loss: 0.0934 - val_accuracy: 0.9709\n",
      "2025-10-23 16:05:04 | INFO | __main__ | Epoch 00057: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:05:04 | INFO | __main__ | tag heure: 2025-10-23_16:05:04 (1761228304.620365)\n",
      "2025-10-23 16:05:08 | INFO | __main__ | Epoch 58/100\n",
      "2025-10-23 16:05:08 | INFO | __main__ | 480/480 - 4s - loss: 0.0372 - accuracy: 0.9907 - val_loss: 0.0937 - val_accuracy: 0.9703\n",
      "2025-10-23 16:05:08 | INFO | __main__ | Epoch 00058: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:05:08 | INFO | __main__ | tag heure: 2025-10-23_16:05:08 (1761228308.7681408)\n",
      "2025-10-23 16:05:12 | INFO | __main__ | Epoch 59/100\n",
      "2025-10-23 16:05:12 | INFO | __main__ | 480/480 - 3s - loss: 0.0372 - accuracy: 0.9909 - val_loss: 0.0931 - val_accuracy: 0.9703\n",
      "2025-10-23 16:05:12 | INFO | __main__ | Epoch 00059: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:05:12 | INFO | __main__ | tag heure: 2025-10-23_16:05:12 (1761228312.0401428)\n",
      "2025-10-23 16:05:15 | INFO | __main__ | Epoch 60/100\n",
      "2025-10-23 16:05:15 | INFO | __main__ | 480/480 - 3s - loss: 0.0371 - accuracy: 0.9909 - val_loss: 0.0935 - val_accuracy: 0.9705\n",
      "2025-10-23 16:05:15 | INFO | __main__ | Epoch 00060: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:05:16 | INFO | __main__ | tag heure: 2025-10-23_16:05:16 (1761228316.0023081)\n",
      "2025-10-23 16:05:19 | INFO | __main__ | Epoch 61/100\n",
      "2025-10-23 16:05:19 | INFO | __main__ | 480/480 - 3s - loss: 0.0371 - accuracy: 0.9909 - val_loss: 0.0935 - val_accuracy: 0.9707\n",
      "2025-10-23 16:05:19 | INFO | __main__ | Epoch 00061: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:05:19 | INFO | __main__ | tag heure: 2025-10-23_16:05:19 (1761228319.9480422)\n",
      "2025-10-23 16:05:23 | INFO | __main__ | Epoch 62/100\n",
      "2025-10-23 16:05:23 | INFO | __main__ | 480/480 - 3s - loss: 0.0369 - accuracy: 0.9907 - val_loss: 0.0934 - val_accuracy: 0.9712\n",
      "2025-10-23 16:05:23 | INFO | __main__ | Epoch 00062: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:05:23 | INFO | __main__ | tag heure: 2025-10-23_16:05:23 (1761228323.2156584)\n",
      "2025-10-23 16:05:26 | INFO | __main__ | Epoch 63/100\n",
      "2025-10-23 16:05:26 | INFO | __main__ | 480/480 - 3s - loss: 0.0369 - accuracy: 0.9910 - val_loss: 0.0933 - val_accuracy: 0.9704\n",
      "2025-10-23 16:05:26 | INFO | __main__ | Epoch 00063: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:05:26 | INFO | __main__ | tag heure: 2025-10-23_16:05:26 (1761228326.7246966)\n",
      "2025-10-23 16:05:31 | INFO | __main__ | Epoch 64/100\n",
      "2025-10-23 16:05:31 | INFO | __main__ | 480/480 - 4s - loss: 0.0368 - accuracy: 0.9909 - val_loss: 0.0933 - val_accuracy: 0.9709\n",
      "2025-10-23 16:05:31 | INFO | __main__ | Epoch 00064: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:05:31 | INFO | __main__ | tag heure: 2025-10-23_16:05:31 (1761228331.0879939)\n",
      "2025-10-23 16:05:35 | INFO | __main__ | Epoch 65/100\n",
      "2025-10-23 16:05:35 | INFO | __main__ | 480/480 - 4s - loss: 0.0368 - accuracy: 0.9909 - val_loss: 0.0931 - val_accuracy: 0.9711\n",
      "2025-10-23 16:05:35 | INFO | __main__ | Epoch 00065: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:05:35 | INFO | __main__ | Epoch 00065: ReduceLROnPlateau reducing learning rate to 6.250000e-03\n",
      "2025-10-23 16:05:35 | INFO | __main__ | tag heure: 2025-10-23_16:05:35 (1761228335.1115632)\n",
      "2025-10-23 16:05:38 | INFO | __main__ | Epoch 66/100\n",
      "2025-10-23 16:05:38 | INFO | __main__ | 480/480 - 3s - loss: 0.0363 - accuracy: 0.9911 - val_loss: 0.0931 - val_accuracy: 0.9708\n",
      "2025-10-23 16:05:38 | INFO | __main__ | Epoch 00066: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:05:38 | INFO | __main__ | tag heure: 2025-10-23_16:05:38 (1761228338.6142285)\n",
      "2025-10-23 16:05:42 | INFO | __main__ | Epoch 67/100\n",
      "2025-10-23 16:05:42 | INFO | __main__ | 480/480 - 3s - loss: 0.0363 - accuracy: 0.9912 - val_loss: 0.0931 - val_accuracy: 0.9707\n",
      "2025-10-23 16:05:42 | INFO | __main__ | Epoch 00067: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:05:42 | INFO | __main__ | tag heure: 2025-10-23_16:05:42 (1761228342.5658362)\n",
      "2025-10-23 16:05:46 | INFO | __main__ | Epoch 68/100\n",
      "2025-10-23 16:05:46 | INFO | __main__ | 480/480 - 4s - loss: 0.0362 - accuracy: 0.9912 - val_loss: 0.0931 - val_accuracy: 0.9708\n",
      "2025-10-23 16:05:46 | INFO | __main__ | Epoch 00068: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:05:46 | INFO | __main__ | tag heure: 2025-10-23_16:05:46 (1761228346.697938)\n",
      "2025-10-23 16:05:50 | INFO | __main__ | Epoch 69/100\n",
      "2025-10-23 16:05:50 | INFO | __main__ | 480/480 - 4s - loss: 0.0362 - accuracy: 0.9912 - val_loss: 0.0933 - val_accuracy: 0.9710\n",
      "2025-10-23 16:05:50 | INFO | __main__ | Epoch 00069: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:05:50 | INFO | __main__ | tag heure: 2025-10-23_16:05:50 (1761228350.9123805)\n",
      "2025-10-23 16:05:54 | INFO | __main__ | Epoch 70/100\n",
      "2025-10-23 16:05:54 | INFO | __main__ | 480/480 - 3s - loss: 0.0361 - accuracy: 0.9911 - val_loss: 0.0934 - val_accuracy: 0.9708\n",
      "2025-10-23 16:05:54 | INFO | __main__ | Epoch 00070: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:05:54 | INFO | __main__ | tag heure: 2025-10-23_16:05:54 (1761228354.2267916)\n",
      "2025-10-23 16:05:58 | INFO | __main__ | Epoch 71/100\n",
      "2025-10-23 16:05:58 | INFO | __main__ | 480/480 - 3s - loss: 0.0361 - accuracy: 0.9913 - val_loss: 0.0934 - val_accuracy: 0.9707\n",
      "2025-10-23 16:05:58 | INFO | __main__ | Epoch 00071: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:05:58 | INFO | __main__ | tag heure: 2025-10-23_16:05:58 (1761228358.0578501)\n",
      "2025-10-23 16:06:01 | INFO | __main__ | Epoch 72/100\n",
      "2025-10-23 16:06:01 | INFO | __main__ | 480/480 - 3s - loss: 0.0361 - accuracy: 0.9912 - val_loss: 0.0933 - val_accuracy: 0.9706\n",
      "2025-10-23 16:06:01 | INFO | __main__ | Epoch 00072: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:06:01 | INFO | __main__ | tag heure: 2025-10-23_16:06:01 (1761228361.7323635)\n",
      "2025-10-23 16:06:05 | INFO | __main__ | Epoch 73/100\n",
      "2025-10-23 16:06:05 | INFO | __main__ | 480/480 - 3s - loss: 0.0361 - accuracy: 0.9913 - val_loss: 0.0932 - val_accuracy: 0.9710\n",
      "2025-10-23 16:06:05 | INFO | __main__ | Epoch 00073: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:06:05 | INFO | __main__ | tag heure: 2025-10-23_16:06:05 (1761228365.4246528)\n",
      "2025-10-23 16:06:09 | INFO | __main__ | Epoch 74/100\n",
      "2025-10-23 16:06:09 | INFO | __main__ | 480/480 - 3s - loss: 0.0361 - accuracy: 0.9913 - val_loss: 0.0933 - val_accuracy: 0.9707\n",
      "2025-10-23 16:06:09 | INFO | __main__ | Epoch 00074: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:06:09 | INFO | __main__ | tag heure: 2025-10-23_16:06:09 (1761228369.1091702)\n",
      "2025-10-23 16:06:13 | INFO | __main__ | Epoch 75/100\n",
      "2025-10-23 16:06:13 | INFO | __main__ | 480/480 - 4s - loss: 0.0360 - accuracy: 0.9913 - val_loss: 0.0934 - val_accuracy: 0.9708\n",
      "2025-10-23 16:06:13 | INFO | __main__ | Epoch 00075: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:06:13 | INFO | __main__ | Epoch 00075: ReduceLROnPlateau reducing learning rate to 3.125000e-03\n",
      "2025-10-23 16:06:13 | INFO | __main__ | tag heure: 2025-10-23_16:06:13 (1761228373.289593)\n",
      "2025-10-23 16:06:17 | INFO | __main__ | Epoch 76/100\n",
      "2025-10-23 16:06:17 | INFO | __main__ | 480/480 - 4s - loss: 0.0358 - accuracy: 0.9915 - val_loss: 0.0933 - val_accuracy: 0.9710\n",
      "2025-10-23 16:06:17 | INFO | __main__ | Epoch 00076: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:06:17 | INFO | __main__ | tag heure: 2025-10-23_16:06:17 (1761228377.393699)\n",
      "2025-10-23 16:06:21 | INFO | __main__ | Epoch 77/100\n",
      "2025-10-23 16:06:21 | INFO | __main__ | 480/480 - 4s - loss: 0.0358 - accuracy: 0.9915 - val_loss: 0.0933 - val_accuracy: 0.9711\n",
      "2025-10-23 16:06:21 | INFO | __main__ | Epoch 00077: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:06:21 | INFO | __main__ | tag heure: 2025-10-23_16:06:21 (1761228381.7680879)\n",
      "2025-10-23 16:06:25 | INFO | __main__ | Epoch 78/100\n",
      "2025-10-23 16:06:25 | INFO | __main__ | 480/480 - 3s - loss: 0.0357 - accuracy: 0.9915 - val_loss: 0.0933 - val_accuracy: 0.9709\n",
      "2025-10-23 16:06:25 | INFO | __main__ | Epoch 00078: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:06:25 | INFO | __main__ | tag heure: 2025-10-23_16:06:25 (1761228385.4420118)\n",
      "2025-10-23 16:06:28 | INFO | __main__ | Epoch 79/100\n",
      "2025-10-23 16:06:28 | INFO | __main__ | 480/480 - 3s - loss: 0.0357 - accuracy: 0.9915 - val_loss: 0.0932 - val_accuracy: 0.9710\n",
      "2025-10-23 16:06:28 | INFO | __main__ | Epoch 00079: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:06:28 | INFO | __main__ | tag heure: 2025-10-23_16:06:28 (1761228388.7580795)\n",
      "2025-10-23 16:06:32 | INFO | __main__ | Epoch 80/100\n",
      "2025-10-23 16:06:32 | INFO | __main__ | 480/480 - 3s - loss: 0.0357 - accuracy: 0.9913 - val_loss: 0.0932 - val_accuracy: 0.9709\n",
      "2025-10-23 16:06:32 | INFO | __main__ | Epoch 00080: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:06:32 | INFO | __main__ | tag heure: 2025-10-23_16:06:32 (1761228392.375163)\n",
      "2025-10-23 16:06:36 | INFO | __main__ | Epoch 81/100\n",
      "2025-10-23 16:06:36 | INFO | __main__ | 480/480 - 3s - loss: 0.0357 - accuracy: 0.9913 - val_loss: 0.0932 - val_accuracy: 0.9709\n",
      "2025-10-23 16:06:36 | INFO | __main__ | Epoch 00081: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:06:36 | INFO | __main__ | tag heure: 2025-10-23_16:06:36 (1761228396.3080206)\n",
      "2025-10-23 16:06:40 | INFO | __main__ | Epoch 82/100\n",
      "2025-10-23 16:06:40 | INFO | __main__ | 480/480 - 3s - loss: 0.0357 - accuracy: 0.9915 - val_loss: 0.0932 - val_accuracy: 0.9712\n",
      "2025-10-23 16:06:40 | INFO | __main__ | Epoch 00082: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:06:40 | INFO | __main__ | tag heure: 2025-10-23_16:06:40 (1761228400.1002605)\n",
      "2025-10-23 16:06:43 | INFO | __main__ | Epoch 83/100\n",
      "2025-10-23 16:06:43 | INFO | __main__ | 480/480 - 3s - loss: 0.0357 - accuracy: 0.9914 - val_loss: 0.0932 - val_accuracy: 0.9712\n",
      "2025-10-23 16:06:43 | INFO | __main__ | Epoch 00083: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:06:43 | INFO | __main__ | tag heure: 2025-10-23_16:06:43 (1761228403.2099366)\n",
      "2025-10-23 16:06:47 | INFO | __main__ | Epoch 84/100\n",
      "2025-10-23 16:06:47 | INFO | __main__ | 480/480 - 4s - loss: 0.0357 - accuracy: 0.9915 - val_loss: 0.0932 - val_accuracy: 0.9711\n",
      "2025-10-23 16:06:47 | INFO | __main__ | Epoch 00084: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:06:47 | INFO | __main__ | tag heure: 2025-10-23_16:06:47 (1761228407.7178555)\n",
      "2025-10-23 16:06:51 | INFO | __main__ | Epoch 85/100\n",
      "2025-10-23 16:06:51 | INFO | __main__ | 480/480 - 4s - loss: 0.0356 - accuracy: 0.9915 - val_loss: 0.0933 - val_accuracy: 0.9710\n",
      "2025-10-23 16:06:51 | INFO | __main__ | Epoch 00085: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:06:51 | INFO | __main__ | Epoch 00085: ReduceLROnPlateau reducing learning rate to 1.562500e-03\n",
      "2025-10-23 16:06:51 | INFO | __main__ | tag heure: 2025-10-23_16:06:51 (1761228411.740089)\n",
      "2025-10-23 16:06:55 | INFO | __main__ | Epoch 86/100\n",
      "2025-10-23 16:06:55 | INFO | __main__ | 480/480 - 4s - loss: 0.0355 - accuracy: 0.9915 - val_loss: 0.0932 - val_accuracy: 0.9710\n",
      "2025-10-23 16:06:55 | INFO | __main__ | Epoch 00086: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:06:55 | INFO | __main__ | tag heure: 2025-10-23_16:06:55 (1761228415.8779032)\n",
      "2025-10-23 16:07:00 | INFO | __main__ | Epoch 87/100\n",
      "2025-10-23 16:07:00 | INFO | __main__ | 480/480 - 4s - loss: 0.0355 - accuracy: 0.9915 - val_loss: 0.0932 - val_accuracy: 0.9710\n",
      "2025-10-23 16:07:00 | INFO | __main__ | Epoch 00087: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:07:00 | INFO | __main__ | tag heure: 2025-10-23_16:07:00 (1761228420.3983154)\n",
      "2025-10-23 16:07:04 | INFO | __main__ | Epoch 88/100\n",
      "2025-10-23 16:07:04 | INFO | __main__ | 480/480 - 4s - loss: 0.0355 - accuracy: 0.9915 - val_loss: 0.0932 - val_accuracy: 0.9712\n",
      "2025-10-23 16:07:04 | INFO | __main__ | Epoch 00088: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:07:04 | INFO | __main__ | tag heure: 2025-10-23_16:07:04 (1761228424.4780009)\n",
      "2025-10-23 16:07:08 | INFO | __main__ | Epoch 89/100\n",
      "2025-10-23 16:07:08 | INFO | __main__ | 480/480 - 3s - loss: 0.0355 - accuracy: 0.9916 - val_loss: 0.0932 - val_accuracy: 0.9712\n",
      "2025-10-23 16:07:08 | INFO | __main__ | Epoch 00089: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:07:08 | INFO | __main__ | tag heure: 2025-10-23_16:07:08 (1761228428.4431438)\n",
      "2025-10-23 16:07:12 | INFO | __main__ | Epoch 90/100\n",
      "2025-10-23 16:07:12 | INFO | __main__ | 480/480 - 3s - loss: 0.0355 - accuracy: 0.9915 - val_loss: 0.0932 - val_accuracy: 0.9712\n",
      "2025-10-23 16:07:12 | INFO | __main__ | Epoch 00090: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:07:12 | INFO | __main__ | tag heure: 2025-10-23_16:07:12 (1761228432.4499514)\n",
      "2025-10-23 16:07:16 | INFO | __main__ | Epoch 91/100\n",
      "2025-10-23 16:07:16 | INFO | __main__ | 480/480 - 4s - loss: 0.0355 - accuracy: 0.9916 - val_loss: 0.0932 - val_accuracy: 0.9712\n",
      "2025-10-23 16:07:16 | INFO | __main__ | Epoch 00091: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:07:16 | INFO | __main__ | tag heure: 2025-10-23_16:07:16 (1761228436.93811)\n",
      "2025-10-23 16:07:21 | INFO | __main__ | Epoch 92/100\n",
      "2025-10-23 16:07:21 | INFO | __main__ | 480/480 - 4s - loss: 0.0355 - accuracy: 0.9916 - val_loss: 0.0932 - val_accuracy: 0.9712\n",
      "2025-10-23 16:07:21 | INFO | __main__ | Epoch 00092: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:07:21 | INFO | __main__ | tag heure: 2025-10-23_16:07:21 (1761228441.1569304)\n",
      "2025-10-23 16:07:24 | INFO | __main__ | Epoch 93/100\n",
      "2025-10-23 16:07:24 | INFO | __main__ | 480/480 - 3s - loss: 0.0355 - accuracy: 0.9915 - val_loss: 0.0932 - val_accuracy: 0.9711\n",
      "2025-10-23 16:07:24 | INFO | __main__ | Epoch 00093: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:07:24 | INFO | __main__ | tag heure: 2025-10-23_16:07:24 (1761228444.9916902)\n",
      "2025-10-23 16:07:28 | INFO | __main__ | Epoch 94/100\n",
      "2025-10-23 16:07:28 | INFO | __main__ | 480/480 - 3s - loss: 0.0355 - accuracy: 0.9915 - val_loss: 0.0932 - val_accuracy: 0.9711\n",
      "2025-10-23 16:07:28 | INFO | __main__ | Epoch 00094: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:07:28 | INFO | __main__ | tag heure: 2025-10-23_16:07:28 (1761228448.520525)\n",
      "2025-10-23 16:07:32 | INFO | __main__ | Epoch 95/100\n",
      "2025-10-23 16:07:32 | INFO | __main__ | 480/480 - 4s - loss: 0.0355 - accuracy: 0.9915 - val_loss: 0.0932 - val_accuracy: 0.9711\n",
      "2025-10-23 16:07:32 | INFO | __main__ | Epoch 00095: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:07:32 | INFO | __main__ | Epoch 00095: ReduceLROnPlateau reducing learning rate to 7.812500e-04\n",
      "2025-10-23 16:07:32 | INFO | __main__ | tag heure: 2025-10-23_16:07:32 (1761228452.816146)\n",
      "2025-10-23 16:07:36 | INFO | __main__ | Epoch 96/100\n",
      "2025-10-23 16:07:36 | INFO | __main__ | 480/480 - 4s - loss: 0.0354 - accuracy: 0.9916 - val_loss: 0.0932 - val_accuracy: 0.9711\n",
      "2025-10-23 16:07:36 | INFO | __main__ | Epoch 00096: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:07:36 | INFO | __main__ | tag heure: 2025-10-23_16:07:36 (1761228456.9561126)\n",
      "2025-10-23 16:07:41 | INFO | __main__ | Epoch 97/100\n",
      "2025-10-23 16:07:41 | INFO | __main__ | 480/480 - 4s - loss: 0.0354 - accuracy: 0.9916 - val_loss: 0.0932 - val_accuracy: 0.9712\n",
      "2025-10-23 16:07:41 | INFO | __main__ | Epoch 00097: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:07:41 | INFO | __main__ | tag heure: 2025-10-23_16:07:41 (1761228461.106358)\n",
      "2025-10-23 16:07:45 | INFO | __main__ | Epoch 98/100\n",
      "2025-10-23 16:07:45 | INFO | __main__ | 480/480 - 4s - loss: 0.0354 - accuracy: 0.9916 - val_loss: 0.0932 - val_accuracy: 0.9711\n",
      "2025-10-23 16:07:45 | INFO | __main__ | Epoch 00098: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:07:45 | INFO | __main__ | tag heure: 2025-10-23_16:07:45 (1761228465.5797088)\n",
      "2025-10-23 16:07:49 | INFO | __main__ | Epoch 99/100\n",
      "2025-10-23 16:07:49 | INFO | __main__ | 480/480 - 3s - loss: 0.0354 - accuracy: 0.9916 - val_loss: 0.0932 - val_accuracy: 0.9711\n",
      "2025-10-23 16:07:49 | INFO | __main__ | Epoch 00099: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:07:49 | INFO | __main__ | tag heure: 2025-10-23_16:07:49 (1761228469.3101437)\n",
      "2025-10-23 16:07:52 | INFO | __main__ | Epoch 100/100\n",
      "2025-10-23 16:07:52 | INFO | __main__ | 480/480 - 2s - loss: 0.0354 - accuracy: 0.9916 - val_loss: 0.0932 - val_accuracy: 0.9711\n",
      "2025-10-23 16:07:52 | INFO | __main__ | Epoch 00100: val_loss did not improve from 0.09304\n",
      "2025-10-23 16:07:52 | INFO | __main__ | Training over.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Evaluation before training (on test set)\")\n",
    "calc_accuracy = True\n",
    "printPerf = True\n",
    "_eval_test = model_eval(\n",
    "    model, test_loader, criterion, compute_device, logger, calc_accuracy, printPerf\n",
    ")\n",
    "\n",
    "logger.info(\"Beginning of training\")\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate, eps=1e-7)\n",
    "\n",
    "if b_use_tensorboard:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "    xp_name = (\n",
    "        str(n_photons)\n",
    "        + \"photons_\"\n",
    "        + str(n_modes)\n",
    "        + \"modes_\"\n",
    "        + str(run_seed)\n",
    "        + \"seed_\"\n",
    "        + str(fold_index)\n",
    "        + \"fold\"\n",
    "    )\n",
    "    tf_train_writer = SummaryWriter(\n",
    "        os.path.join(run_dir, \"runs/\" + xp_name + \"_train\")\n",
    "    )\n",
    "    tf_val_writer = SummaryWriter(os.path.join(run_dir, \"runs/\" + xp_name + \"_val\"))\n",
    "else:\n",
    "    tf_train_writer = None\n",
    "    tf_val_writer = None\n",
    "\n",
    "early_stop_patience = n_epochs\n",
    "early_stop_min_delta = 0.000001\n",
    "b_use_cosine_scheduler = False\n",
    "[\n",
    "    train_loss_history,\n",
    "    train_accuracy_history,\n",
    "    val_loss_history,\n",
    "    val_accuracy_history,\n",
    "    duree_totale,\n",
    "    best_val_epoch,\n",
    "] = model_fit(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    n_epochs,\n",
    "    os.path.join(run_dir, f_out_weights),\n",
    "    early_stop_patience,\n",
    "    early_stop_min_delta,\n",
    "    reduce_lr_patience,\n",
    "    reduce_lr_factor,\n",
    "    compute_device,\n",
    "    logger,\n",
    "    b_use_cosine_scheduler,\n",
    "    tf_train_writer=tf_train_writer,\n",
    "    tf_val_writer=tf_val_writer,\n",
    "    calc_accuracy=calc_accuracy,\n",
    ")\n",
    "\n",
    "logger.info(\"Training over.\")\n",
    "n_train_epochs = len(train_loss_history)\n",
    "time_t4 = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, the best model (selected based on the lowest validation loss) is loaded, and its accuracy is evaluated on the training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T21:13:22.800309Z",
     "start_time": "2025-08-29T21:13:22.738296Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 16:08:21 | INFO | __main__ | Final evaluation (on test set)\n",
      "2025-10-23 16:08:21 | INFO | __main__ | n_model_input_features: {n_model_input_features}\n",
      "2025-10-23 16:08:23 | INFO | __main__ | 480/480 - 1s - loss: 0.0383 - accuracy: 0.9906\n",
      "2025-10-23 16:08:24 | INFO | __main__ | 120/120 - 0s - loss: 0.0930 - accuracy: 0.9715\n",
      "2025-10-23 16:08:24 | INFO | __main__ | 100/100 - 0s - loss: 0.0937 - accuracy: 0.9710\n",
      "2025-10-23 16:08:24 | INFO | __main__ | Duration - Quantum layer creation: 36.24s\n",
      "2025-10-23 16:08:24 | INFO | __main__ | Duration - Quantum features encoding: 7.53s\n",
      "2025-10-23 16:08:24 | INFO | __main__ | Duration - training: 458.3s\n",
      "2025-10-23 16:08:24 | INFO | __main__ | Duration - total: 534.15s\n",
      "2025-10-23 16:08:24 | INFO | __main__ | Best val epoch: 45\n",
      "2025-10-23 16:08:24 | INFO | __main__ | Accuracies - train: 0.9906, val: 0.9715, test: 0.9710\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Final evaluation (on test set)\")\n",
    "best_state_dict = torch.load(\n",
    "    os.path.join(run_dir, f_out_weights), map_location=compute_device\n",
    ")\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    logger.info(\"n_model_input_features: {n_model_input_features}\")\n",
    "    [_, train_acc, _] = model_eval(\n",
    "        model,\n",
    "        train_loader,\n",
    "        criterion,\n",
    "        compute_device,\n",
    "        logger,\n",
    "        calc_accuracy,\n",
    "        printPerf,\n",
    "    )\n",
    "    train_acc = int(1000000.0 * train_acc.item()) / 1000000.0\n",
    "    [_, val_acc, _] = model_eval(\n",
    "        model,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        compute_device,\n",
    "        logger,\n",
    "        calc_accuracy,\n",
    "        printPerf,\n",
    "    )\n",
    "    val_acc = int(1000000.0 * val_acc.item()) / 1000000.0\n",
    "    [_, test_acc, _] = model_eval(\n",
    "        model,\n",
    "        test_loader,\n",
    "        criterion,\n",
    "        compute_device,\n",
    "        logger,\n",
    "        calc_accuracy,\n",
    "        printPerf,\n",
    "    )\n",
    "    test_acc = int(1000000.0 * test_acc.item()) / 1000000.0\n",
    "except RuntimeError as e:\n",
    "    logger.info(f\"Error while loading state_dict : {e}\")\n",
    "    train_acc = float(\"nan\")\n",
    "    val_acc = float(\"nan\")\n",
    "    test_acc = float(\"nan\")\n",
    "time_t5 = time.time()\n",
    "\n",
    "duration_creation_couche_quantique = int(100.0 * (time_t2 - time_t1)) / 100.0\n",
    "logger.info(\n",
    "    \"Duration - Quantum layer creation: {}s\".format(\n",
    "        duration_creation_couche_quantique\n",
    "    )\n",
    ")\n",
    "duration_calcul_quantum_features = int(100.0 * (time_t3 - time_t2)) / 100.0\n",
    "logger.info(\n",
    "    \"Duration - Quantum features encoding: {}s\".format(\n",
    "        duration_calcul_quantum_features\n",
    "    )\n",
    ")\n",
    "duration_qfeatures = (\n",
    "    duration_creation_couche_quantique + duration_calcul_quantum_features\n",
    ")\n",
    "duration_train = int(100.0 * (time_t4 - time_t3)) / 100.0\n",
    "logger.info(\"Duration - training: {}s\".format(duration_train))\n",
    "duration_totale = int(100.0 * (time_t5 - time_t1)) / 100.0\n",
    "logger.info(\"Duration - total: {}s\".format(duration_totale))\n",
    "logger.info(\"Best val epoch: {}\".format(best_val_epoch))\n",
    "\n",
    "# Afficher les accuracies après apprentissage\n",
    "logger.info(\"Accuracies - train: {:.4f}, val: {:.4f}, test: {:.4f}\".format(\n",
    "    train_acc,\n",
    "    val_acc,\n",
    "    test_acc\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook successfully **reproduces and occasionally surpasses** the reference results (**99.06% train accuracy, 97.10% test accuracy** with 3 photons and 20 modes), serving as a **proof of concept** for the advantages of **quantum feature extraction** in machine learning.\n",
    "\n",
    "### Key Findings\n",
    "- **Performance Validation**: The hybrid QORC model (quantum reservoir + linear classifier) matches the original benchmarks, confirming the **effectiveness of quantum features** for MNIST classification.\n",
    "- **Quantum Advantage**: The photonic circuit's ability to generate **highly non-linear features**—intractable for classical methods—demonstrates the potential of quantum-enhanced approaches.\n",
    "- **Scalability**: While this experiment uses a modest configuration (3 photons, 20 modes), the exponential scaling of the Fock space suggests even greater benefits for larger systems.\n",
    "- **NISQ Compatibility**:\n",
    "   The QORC architecture is inherently **robust to noise**, as it relies on **repeatable quantum feature patterns** rather than precise quantum state fidelity. This makes it a promising candidate for deployment on **current-generation NISQ (Noisy Intermediate-Scale Quantum) devices**, where systematic errors can be mitigated through statistical repetition.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "The notebook was tested with the following library versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.12 | packaged by conda-forge | (main, Oct 13 2025, 14:23:59) [MSC v.1944 64 bit (AMD64)]\n",
      "NumPy: 2.3.3\n",
      "Scikit-learn: 1.7.1\n",
      "Matplotlib: 3.10.7\n",
      "PyTorch: 2.7.1+cpu\n",
      "Perceval: 0.13.2\n",
      "MerLin: 0.1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import matplotlib\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"Matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Perceval: {pcvl.__version__}\")\n",
    "print(f\"MerLin: {ML.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Python: 3.12.12 | packaged by conda-forge | (main, Oct 13 2025, 14:23:59) [MSC v.1944 64 bit (AMD64)]\n",
    "NumPy: 2.3.3\n",
    "Scikit-learn: 1.7.1\n",
    "Matplotlib: 3.10.7\n",
    "PyTorch: 2.7.1+cpu\n",
    "Perceval: 0.13.2\n",
    "MerLin: 0.1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
